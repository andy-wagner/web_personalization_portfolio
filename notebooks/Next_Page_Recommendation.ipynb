{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Next_Page_Recommendation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaime-garvey/web_personalization_portfolio/blob/master/notebooks/Next_Page_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PDlqZkswvK",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34AIJymyJjmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQydi0t7s8NX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dtj1oeMs0uK",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHyof4hlnu3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#id_id2page_dict = \"1DGeJYGuF5fTnruZz84ZaBGelakRcKiKf\"\n",
        "id_possible_actions_dict = \"15WUaciYffXqNN9_3t3TGWXJ5FotPWtx0\"\n",
        "#id_policy= \"1yiYN7TVackl0ktiiWSIFy7viVdiy7ffq\"\n",
        "id_V = \"16RDEMUNEJOWHMlmAgZWL0zJ1jgx2_F2v\"\n",
        "id_rewards_dict = \"1CPKK-kriTdUY1VtWfdCAA3HvZklEa116\"\n",
        "id_bounce = \"1_L0Jvd2eZGB4IyFUdI_omjCW3NRIOWOe\"\n",
        "#id_terminals = \"1uN6U81lbLi7QKDABcSue5jYluTzyoF3z\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvCl1rzVtdN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_pickle_from_drive(id):\n",
        "  downloaded = drive.CreateFile({'id':id}) \n",
        "  downloaded.GetContentFile('Filename.csv')  \n",
        "  data = pickle.load(open('Filename.csv', 'rb'))\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjfzqG2ruirc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bounce_dict = load_pickle_from_drive(id_bounce)\n",
        "possible_actions= load_pickle_from_drive(id_possible_actions_dict)\n",
        "#policy = load_pickle_from_drive(id_policy)\n",
        "V = load_pickle_from_drive(id_V)\n",
        "rewards_dict = load_pickle_from_drive(id_rewards_dict)\n",
        "#terminals = load_pickle_from_drive(id_terminals)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_dCSgIWOYKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terminals = [k for k,v in rewards_dict.items() if v == 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls8DlmBmIP8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "possible_actions = possible_actions.to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXBLCcMRUfxD",
        "colab_type": "text"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldzaGWY8fxws",
        "colab_type": "code",
        "outputId": "7c8e746c-e3e8-4c97-9945-d90598523b31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1095
        }
      },
      "source": [
        "possible_actions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "state1\n",
              "20 oz Stainless Steel Insulated Tumbler                               {'Drinkware': 8.853474988933157e-05, 'Electron...\n",
              "26 oz Double Wall Insulated Bottle                                    {'Android Glass Water Bottle with Black Sleeve...\n",
              "Accessories                                                           {'Accessories': 0.0024789729969012836, 'Fun': ...\n",
              "Android                                                               {'Android': 0.003806994245241257, 'Android Men...\n",
              "Android Glass Water Bottle with Black Sleeve                          {'Android Glass Water Bottle with Black Sleeve...\n",
              "Android Men's Vintage Henley                                          {'Android': 8.853474988933157e-05, 'Android Me...\n",
              "Android Men's Vintage Tee                                             {'Android': 8.853474988933157e-05, 'Men's-T-Sh...\n",
              "Android Men's Zip Hoodie                                                        {'Kid's-Infant': 8.853474988933157e-05}\n",
              "Android Rise 14 oz Mug                                                {'Drinkware': 0.00017706949977866314, 'The Goo...\n",
              "Android Toddler Short Sleeve T-shirt Pewter                                           {'Kid's': 0.00017706949977866314}\n",
              "Android Toddler Short Sleeve T-shirt Pink                             {'Kid's-Infant': 8.853474988933157e-05, 'Kid's...\n",
              "Android Women's Fleece Hoodie                                         {'Google Women's Performance Full Zip Jacket B...\n",
              "Android Women's Short Sleeve Hero Tee Black                                 {'Women's-T-Shirts': 8.853474988933157e-05}\n",
              "Android Youth Short Sleeve T-shirt Pewter                                              {'Kid's': 8.853474988933157e-05}\n",
              "Apparel                                                               {'Android Toddler Short Sleeve T-shirt Pink': ...\n",
              "Backpacks                                                             {'Apparel': 0.0003541389995573263, 'Backpacks'...\n",
              "Badge Holder                                                          {'Google': 8.853474988933157e-05, 'Office': 8....\n",
              "Bags                                                                  {'Android': 0.00017706949977866314, 'Apparel':...\n",
              "Bic Intensity Clic Gel Pen                                                           {'Office': 0.00017706949977866314}\n",
              "Bic Leather Pen                                                                       {'Office': 0.0003541389995573263}\n",
              "Brands                                                                {'Bags': 8.853474988933157e-05, 'Brands': 0.00...\n",
              "Checkout Confirmation                                                 {'Checkout Confirmation': 0.003010181496237273...\n",
              "Checkout Review                                                       {'Checkout Confirmation': 0.002921646746347941...\n",
              "Checkout Your Information                                             {'Checkout Your Information': 0.00168216024789...\n",
              "Color Changing Grip Pen                                               {'Office': 0.00017706949977866314, 'Shopping C...\n",
              "Compact Eco Journal                                                   {'Office': 0.00017706949977866314, 'YouTube': ...\n",
              "Drinkware                                                             {'20 oz Stainless Steel Insulated Tumbler': 0....\n",
              "Electronics                                                           {'Android': 0.0003541389995573263, 'Apparel': ...\n",
              "Frequently Asked Questions                                            {'Bags': 8.853474988933157e-05, 'Google Men's ...\n",
              "Fun                                                                   {'Accessories': 0.0005312084993359894, 'Fun': ...\n",
              "                                                                                            ...                        \n",
              "The Google Merchandise Store/Metal Earbuds with Small Zipper Case     {'Drinkware': 8.853474988933157e-05, 'Electron...\n",
              "The Google Merchandise Store/Mistral Rucksack                         {'Android': 8.853474988933157e-05, 'Apparel': ...\n",
              "The Google Merchandise Store/Oasis Backpack                           {'Backpacks': 0.00017706949977866314, 'Bags': ...\n",
              "The Google Merchandise Store/PaperMate Ink Joy RT Pen                 {'Office': 0.00017706949977866314, 'YouTube': ...\n",
              "The Google Merchandise Store/Pen Pencil & Highlighter Set             {'Electronics': 8.853474988933157e-05, 'Google...\n",
              "The Google Merchandise Store/Plastic Sliding Flashlight               {'Android': 8.853474988933157e-05, 'Electronic...\n",
              "The Google Merchandise Store/Pop-a-Point Crayon                                       {'Office': 8.853474988933157e-05}\n",
              "The Google Merchandise Store/PowerClip Lightning Charger              {'Electronics': 0.0002656042496679947, 'Shoppi...\n",
              "The Google Merchandise Store/Red Spiral Google Notebook               {'Office': 0.00017706949977866314, 'The Google...\n",
              "The Google Merchandise Store/Rocket Flashlight                        {'Electronics': 8.853474988933157e-05, 'Return...\n",
              "The Google Merchandise Store/Solo Pro Backpack                        {'Bags': 0.0002656042496679947, 'Electronics':...\n",
              "The Google Merchandise Store/Straw Beach Mat                                          {'Office': 8.853474988933157e-05}\n",
              "The Google Merchandise Store/Svanasana Yoga Block                                  {'Lifestyle': 8.853474988933157e-05}\n",
              "The Google Merchandise Store/UFO Bluetooth Water Resistant Speaker    {'Electronics': 0.00017706949977866314, 'Shopp...\n",
              "The Google Merchandise Store/Waterpoof Gear Bag                                         {'Home': 8.853474988933157e-05}\n",
              "The Google Merchandise Store/Windup Android                                           {'Office': 8.853474988933157e-05}\n",
              "Vestiti-T-shirt                                                       {'Shopping Cart': 8.853474988933157e-05, 'Vest...\n",
              "Wearables                                                             {'Apparel': 8.853474988933157e-05, 'Brands': 0...\n",
              "Women's                                                               {'Android': 8.853474988933157e-05, 'Apparel': ...\n",
              "Women's-Outerwear                                                     {'Apparel': 8.853474988933157e-05, 'Bags': 0.0...\n",
              "Women's-T-Shirts                                                      {'Android': 8.853474988933157e-05, 'Android Wo...\n",
              "Yoga Mat                                                              {'Lifestyle': 8.853474988933157e-05, 'Office':...\n",
              "YouTube                                                               {'Android': 0.0006197432492253209, 'Apparel': ...\n",
              "YouTube Men's 3/4 Sleeve Henley                                                {'Shopping Cart': 8.853474988933157e-05}\n",
              "YouTube Men's Short Sleeve Hero Tee Black                                            {'YouTube': 8.853474988933157e-05}\n",
              "YouTube Men's Short Sleeve Hero Tee White                                      {'Shopping Cart': 8.853474988933157e-05}\n",
              "YouTube Men's Vintage Tank                                                           {'YouTube': 8.853474988933157e-05}\n",
              "YouTube Trucker Hat                                                   {'Home': 8.853474988933157e-05, 'YouTube': 8.8...\n",
              "Your Wishlist                                                         {'Google Women's Hero V-Neck Tee White': 8.853...\n",
              "Дома                                                                  {'Men's-T-Shirts': 8.853474988933157e-05, 'You...\n",
              "Length: 176, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJG_182Itlkr",
        "colab_type": "text"
      },
      "source": [
        "# Recommender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdA0FtyKDEAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Gstore:\n",
        "  '''\n",
        "  state_0 --> initial state\n",
        "  terminal_states --> terminal state\n",
        "  possible_actions --> all actions for every state {state1: {state2, tp}...}\n",
        "  '''\n",
        "  \n",
        "  possible_actions = possible_actions\n",
        "  bounce_dict=bounce_dict\n",
        "  \n",
        "  def __init__(self, state, actions=None, scores= None, reward= None, terminals = terminals, rewards_dict=rewards_dict, V=V, bounce_dict=bounce_dict):\n",
        "    self.state = state\n",
        "    self.scores = {}\n",
        "    self.reward = rewards_dict[self.state]\n",
        "    self.actions = possible_actions[self.state]\n",
        "    self.rewards_dict = rewards_dict\n",
        "    self.V = V\n",
        "    \n",
        "  def get_recommendation(self):\n",
        "    #takes in state and returns list of possible actions if not a terminal state\n",
        "    \n",
        "    #check if state isn't terminal\n",
        "    if self.rewards_dict[self.state] != 1:\n",
        "      \n",
        "      def key_maxscore():\n",
        "        v=list(self.actions.values())\n",
        "        k=list(self.actions.keys())\n",
        "        return k[v.index(max(v))]\n",
        "      \n",
        "      for a, prob in self.actions.items():\n",
        "        v = V[a]\n",
        "        b = bounce_dict[a]\n",
        "        score = (1-b) * prob * v\n",
        "        \n",
        "        self.scores[a] = score\n",
        "        \n",
        "        return key_maxscore()\n",
        "    \n",
        "    \n",
        "    else:\n",
        "      return \"Your Wishlist\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv7nySwpV7oD",
        "colab_type": "code",
        "outputId": "dac594ac-ace0-41ae-bb01-2c99d75d65f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Gstore(\"Google Women's Scoop Neck Tee White\").get_recommendation()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Apparel'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqMx94seYyKr",
        "colab_type": "text"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loWC8LltgP1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Helper Function to itialize Value Function\n",
        "\n",
        "def initialize_values(rewards_dict=rewards_dict):\n",
        "    #function to inital values at 0\n",
        "    # V(s) only has value if it's not a terminal state\n",
        "    \n",
        "    V={}\n",
        "    for state in list(rewards_dict.keys()):\n",
        "      if rewards_dict[state] == 1:\n",
        "        V[state] = 0\n",
        "      else:\n",
        "        V[state] = np.random.random()\n",
        "    \n",
        "    return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aiiVTdJntK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_policy(possible_actions=possible_actions, terminals=terminals):\n",
        "  policy = {}\n",
        "  \n",
        "  def key_maxval(state):\n",
        "      \n",
        "      v=list(possible_actions[state].values())\n",
        "      k=list(possible_actions[state].keys())\n",
        "  \n",
        "      return k[v.index(max(v))]\n",
        "    \n",
        "  for state in list(possible_actions.keys()):\n",
        "    #if state not in terminals:\n",
        "    policy[state] = key_maxval(state)\n",
        "    \n",
        "  return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKPiBZwlp9kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize Policy (state --> action)\n",
        "policy = initialize_policy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5nrVUEBrNKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize Value (future rewards)\n",
        "V = initialize_values()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RFTMy1419j1",
        "colab_type": "code",
        "outputId": "39af146f-9931-4a36-9b37-9204f057d709",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(list(policy.keys()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAXxlCx0raRA",
        "colab_type": "text"
      },
      "source": [
        "# Value Iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwbKhPe28wVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(V=V, gamma=0.9, epsilon=0.001):\n",
        "  while True:\n",
        "    \n",
        "    old_V = V.copy()\n",
        "    delta = 0\n",
        "    # V(s) only has value if it's not a terminal state\n",
        "    for state in list(policy.keys()):\n",
        "      new_v = float('-inf')\n",
        "      try:\n",
        "        #loop and take max value for action \n",
        "        for action in list(MDP(state).actions().keys()):\n",
        "          r = MDP(action).reward()\n",
        "          v = r + gamma * V[action]\n",
        "          if v > new_v:\n",
        "            new_v = v\n",
        "        V[state] = new_v\n",
        "        delta = max(delta, np.abs(old_v = V[state]))\n",
        "        \n",
        "      except:\n",
        "        pass\n",
        "    if delta < epsilon:\n",
        "      break\n",
        "         \n",
        "    #find optimal policy\n",
        "        \n",
        "    for state in list(policy.keys()):\n",
        "      best_action = None\n",
        "      best_value = float('-inf')\n",
        "            \n",
        "            \n",
        "      # loop through all possible actions to find the best current action\n",
        "      try:\n",
        "        for action in list(MDP(state).actions().keys()):\n",
        "          v = r + gamma * V[action]\n",
        "          r = MDP(action).reward()\n",
        "\n",
        "          if v>best_value:\n",
        "            best_value=v\n",
        "            best_a = action\n",
        "        policy[state] = best_a\n",
        "      except:\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moHXqUVk-Ppv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(V=V, gamma=0.9, theta=0.001):\n",
        "  while True:\n",
        "    \n",
        "    old_V = V.copy()\n",
        "    delta = 0\n",
        "    counter = 0\n",
        "    # V(s) only has value if it's not a terminal state\n",
        "    for state in list(policy.keys()):\n",
        "      counter += 1\n",
        "      print(\"Iteration numer....{}\".format(counter))\n",
        "      new_v = float('-inf')\n",
        "      try:\n",
        "        #loop and take max value for action \n",
        "        for action in list(MDP(state).actions().keys()):\n",
        "          r = MDP(action).reward()\n",
        "          v = r + gamma * V[action]\n",
        "          if v > new_v:\n",
        "            new_v = v\n",
        "        V[state] = new_v\n",
        "        delta = max(delta, np.abs(old_v = V[state]))\n",
        "        \n",
        "      except:\n",
        "        pass\n",
        "    if delta < theta:\n",
        "      break\n",
        "         \n",
        "    #find optimal policy\n",
        "        \n",
        "    for state in list(policy.keys()):\n",
        "      best_action = None\n",
        "      best_value = float('-inf')\n",
        "            \n",
        "            \n",
        "      # loop through all possible actions to find the best current action\n",
        "      try:\n",
        "        for action in list(MDP(state).actions().keys()):\n",
        "          v = r + gamma * V[action]\n",
        "          r = MDP(action).reward()\n",
        "\n",
        "          if v>best_value:\n",
        "            best_value=v\n",
        "            best_a = action\n",
        "        policy[state] = best_a\n",
        "      except:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS15EWOAzbtI",
        "colab_type": "code",
        "outputId": "384c661a-c5f5-4d49-ca14-02a940876714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2974
        }
      },
      "source": [
        "value_iteration()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration numer....1\n",
            "Iteration numer....2\n",
            "Iteration numer....3\n",
            "Iteration numer....4\n",
            "Iteration numer....5\n",
            "Iteration numer....6\n",
            "Iteration numer....7\n",
            "Iteration numer....8\n",
            "Iteration numer....9\n",
            "Iteration numer....10\n",
            "Iteration numer....11\n",
            "Iteration numer....12\n",
            "Iteration numer....13\n",
            "Iteration numer....14\n",
            "Iteration numer....15\n",
            "Iteration numer....16\n",
            "Iteration numer....17\n",
            "Iteration numer....18\n",
            "Iteration numer....19\n",
            "Iteration numer....20\n",
            "Iteration numer....21\n",
            "Iteration numer....22\n",
            "Iteration numer....23\n",
            "Iteration numer....24\n",
            "Iteration numer....25\n",
            "Iteration numer....26\n",
            "Iteration numer....27\n",
            "Iteration numer....28\n",
            "Iteration numer....29\n",
            "Iteration numer....30\n",
            "Iteration numer....31\n",
            "Iteration numer....32\n",
            "Iteration numer....33\n",
            "Iteration numer....34\n",
            "Iteration numer....35\n",
            "Iteration numer....36\n",
            "Iteration numer....37\n",
            "Iteration numer....38\n",
            "Iteration numer....39\n",
            "Iteration numer....40\n",
            "Iteration numer....41\n",
            "Iteration numer....42\n",
            "Iteration numer....43\n",
            "Iteration numer....44\n",
            "Iteration numer....45\n",
            "Iteration numer....46\n",
            "Iteration numer....47\n",
            "Iteration numer....48\n",
            "Iteration numer....49\n",
            "Iteration numer....50\n",
            "Iteration numer....51\n",
            "Iteration numer....52\n",
            "Iteration numer....53\n",
            "Iteration numer....54\n",
            "Iteration numer....55\n",
            "Iteration numer....56\n",
            "Iteration numer....57\n",
            "Iteration numer....58\n",
            "Iteration numer....59\n",
            "Iteration numer....60\n",
            "Iteration numer....61\n",
            "Iteration numer....62\n",
            "Iteration numer....63\n",
            "Iteration numer....64\n",
            "Iteration numer....65\n",
            "Iteration numer....66\n",
            "Iteration numer....67\n",
            "Iteration numer....68\n",
            "Iteration numer....69\n",
            "Iteration numer....70\n",
            "Iteration numer....71\n",
            "Iteration numer....72\n",
            "Iteration numer....73\n",
            "Iteration numer....74\n",
            "Iteration numer....75\n",
            "Iteration numer....76\n",
            "Iteration numer....77\n",
            "Iteration numer....78\n",
            "Iteration numer....79\n",
            "Iteration numer....80\n",
            "Iteration numer....81\n",
            "Iteration numer....82\n",
            "Iteration numer....83\n",
            "Iteration numer....84\n",
            "Iteration numer....85\n",
            "Iteration numer....86\n",
            "Iteration numer....87\n",
            "Iteration numer....88\n",
            "Iteration numer....89\n",
            "Iteration numer....90\n",
            "Iteration numer....91\n",
            "Iteration numer....92\n",
            "Iteration numer....93\n",
            "Iteration numer....94\n",
            "Iteration numer....95\n",
            "Iteration numer....96\n",
            "Iteration numer....97\n",
            "Iteration numer....98\n",
            "Iteration numer....99\n",
            "Iteration numer....100\n",
            "Iteration numer....101\n",
            "Iteration numer....102\n",
            "Iteration numer....103\n",
            "Iteration numer....104\n",
            "Iteration numer....105\n",
            "Iteration numer....106\n",
            "Iteration numer....107\n",
            "Iteration numer....108\n",
            "Iteration numer....109\n",
            "Iteration numer....110\n",
            "Iteration numer....111\n",
            "Iteration numer....112\n",
            "Iteration numer....113\n",
            "Iteration numer....114\n",
            "Iteration numer....115\n",
            "Iteration numer....116\n",
            "Iteration numer....117\n",
            "Iteration numer....118\n",
            "Iteration numer....119\n",
            "Iteration numer....120\n",
            "Iteration numer....121\n",
            "Iteration numer....122\n",
            "Iteration numer....123\n",
            "Iteration numer....124\n",
            "Iteration numer....125\n",
            "Iteration numer....126\n",
            "Iteration numer....127\n",
            "Iteration numer....128\n",
            "Iteration numer....129\n",
            "Iteration numer....130\n",
            "Iteration numer....131\n",
            "Iteration numer....132\n",
            "Iteration numer....133\n",
            "Iteration numer....134\n",
            "Iteration numer....135\n",
            "Iteration numer....136\n",
            "Iteration numer....137\n",
            "Iteration numer....138\n",
            "Iteration numer....139\n",
            "Iteration numer....140\n",
            "Iteration numer....141\n",
            "Iteration numer....142\n",
            "Iteration numer....143\n",
            "Iteration numer....144\n",
            "Iteration numer....145\n",
            "Iteration numer....146\n",
            "Iteration numer....147\n",
            "Iteration numer....148\n",
            "Iteration numer....149\n",
            "Iteration numer....150\n",
            "Iteration numer....151\n",
            "Iteration numer....152\n",
            "Iteration numer....153\n",
            "Iteration numer....154\n",
            "Iteration numer....155\n",
            "Iteration numer....156\n",
            "Iteration numer....157\n",
            "Iteration numer....158\n",
            "Iteration numer....159\n",
            "Iteration numer....160\n",
            "Iteration numer....161\n",
            "Iteration numer....162\n",
            "Iteration numer....163\n",
            "Iteration numer....164\n",
            "Iteration numer....165\n",
            "Iteration numer....166\n",
            "Iteration numer....167\n",
            "Iteration numer....168\n",
            "Iteration numer....169\n",
            "Iteration numer....170\n",
            "Iteration numer....171\n",
            "Iteration numer....172\n",
            "Iteration numer....173\n",
            "Iteration numer....174\n",
            "Iteration numer....175\n",
            "Iteration numer....176\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KA9o2ed9jgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = open('policy.pkl', 'wb')\n",
        "pickle.dump(policy,output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFYnx9UjAJvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = open('V.pkl', 'wb')\n",
        "pickle.dump(V,output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiWLQZ9Gavxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_lookahead(state=None, V=None, discount_factor = 0.9):\n",
        "    \"\"\"\n",
        "    Helper function to  calculate state-value function\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object\n",
        "        state: state to consider\n",
        "        V: Estimated Value for each state. Vector of length nS\n",
        "        discount_factor: MDP discount factor\n",
        "        \n",
        "    Return:\n",
        "        action_values: Expected value of each action in a state. Vector of length nA\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize vector of action values\n",
        "    V = initialize_values()\n",
        "    \n",
        "    # loop over the actions we can take in an enviorment \n",
        "    for action,probability in MDP(state).actions().items():\n",
        "      reward = MDP(action)\n",
        "      \n",
        "        # loop over the P_sa distribution.\n",
        "        for probablity, next_state, reward, info in env.P[state][action]:\n",
        "             #if we are in state s and take action a. then sum over all the possible states we can land into.\n",
        "            action_values[action] += probablity * (reward + (discount_factor * V[next_state]))\n",
        "            \n",
        "    return action_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plo5uHNYi4On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_policy(env, policy, V, discount_factor):\n",
        "    \n",
        "    \"\"\"\n",
        "    Helper function to update a given policy based on given value function.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        policy: policy to update.\n",
        "        V: Estimated Value for each state. Vector of length nS.\n",
        "        discount_factor: MDP discount factor.\n",
        "    Return:\n",
        "        policy: Updated policy based on the given state-Value function 'V'.\n",
        "    \"\"\"\n",
        "    \n",
        "    for state in range(env.nS):\n",
        "        # for a given state compute state-action value.\n",
        "        action_values = one_step_lookahead(env, state, V, discount_factor)\n",
        "        \n",
        "        # choose the action which maximizez the state-action value.\n",
        "        policy[state] =  np.argmax(action_values)\n",
        "        \n",
        "    return policy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulperLWsi53n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(env, discount_factor = 0.999, max_iteration = 1000):\n",
        "    \"\"\"\n",
        "    Algorithm to solve MPD.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        discount_factor: MDP discount factor.\n",
        "        max_iteration: Maximum No.  of iterations to run.\n",
        "        \n",
        "    Return:\n",
        "        V: Optimal state-Value function. Vector of lenth nS.\n",
        "        optimal_policy: Optimal policy. Vector of length nS.\n",
        "    \n",
        "    \"\"\"\n",
        "    # intialize value fucntion\n",
        "    V = np.zeros(env.nS)\n",
        "    \n",
        "    # iterate over max_iterations\n",
        "    for i in range(max_iteration):\n",
        "        \n",
        "        #  keep track of change with previous value function\n",
        "        prev_v = np.copy(V) \n",
        "    \n",
        "        # loop over all states\n",
        "        for state in range(env.nS):\n",
        "            \n",
        "            # Asynchronously update the state-action value\n",
        "            #action_values = one_step_lookahead(env, state, V, discount_factor)\n",
        "            \n",
        "            # Synchronously update the state-action value\n",
        "            action_values = one_step_lookahead(env, state, prev_v, discount_factor)\n",
        "            \n",
        "            # select best action to perform based on highest state-action value\n",
        "            best_action_value = np.max(action_values)\n",
        "            \n",
        "            # update the current state-value fucntion\n",
        "            V[state] =  best_action_value\n",
        "            \n",
        "        # if policy not changed over 10 iterations it converged.\n",
        "        if i % 10 == 0:\n",
        "            # if values of 'V' not changing after one iteration\n",
        "            if (np.all(np.isclose(V, prev_v))):\n",
        "                print('Value converged at iteration %d' %(i+1))\n",
        "                break\n",
        "\n",
        "    # intialize optimal policy\n",
        "    optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
        "    \n",
        "    # update the optimal polciy according to optimal value function 'V'\n",
        "    optimal_policy = update_policy(env, optimal_policy, V, discount_factor)\n",
        "    \n",
        "    return V, optimal_policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoPoRazBixtD",
        "colab_type": "text"
      },
      "source": [
        "#### CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvJPjGux3f_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Value(MDP):\n",
        "  '''\n",
        "  states_actions --> dictionary {state_i: {state_j : transition_probability}}\n",
        "  id2pages --> dictionary {page id: \"Page Title\"}\n",
        "  rewards --> dictionary {state: reward value}\n",
        "  theta --> threshold for the amount of iterations\n",
        "  \n",
        "  \n",
        "  '''\n",
        "  V = initialize_values()\n",
        "  \n",
        "  def __init__(self, theta=0.00001):\n",
        "\n",
        "    super().__init__(state_i, possible_actions, terminal_states, gamma)\n",
        "    \n",
        "    self.theta = threshold\n",
        "    self.gamma = gamma\n",
        "    \n",
        "  def initialize_values(self):\n",
        "    #function to inital values at 0\n",
        "    # V(s) only has value if it's not a terminal state\n",
        "    \n",
        "    for page in id2pages.key:\n",
        "      \n",
        "      if page in actions():\n",
        "        \n",
        "        self.V[page] = np.random.random()\n",
        "      else:\n",
        "        self.V[page] = 0      #terminal state\n",
        "        \n",
        "      return self.V\n",
        "  \n",
        "  \n",
        "  def state_action(self, state, gamma):\n",
        "    #Helper function to caluclate Q --> state_value function\n",
        "    \n",
        "    \n",
        "    for action in range(length(self.possible_actions)):\n",
        "      #loop though all possible actions\n",
        "      \n",
        "      for probability, next_state, reward in statespace??\n",
        "      #if we are in state s and take action a. then sum over all \n",
        "      #the possible states we can land into.\n",
        "      \n",
        "        state_action[action] += probability * (reward + (discount_factor * V[next_state]))\n",
        "        \n",
        "    return q\n",
        "        \n",
        "      \n",
        "  def update_policy(self, policy, V, gamma):\n",
        "    #Function to update a given policy based on a given value function\n",
        "    \n",
        "    for state in range(states.keys):\n",
        "      action_values = q\n",
        "      \n",
        "      policy[state] = np.argmax(action_values)\n",
        "    \n",
        "      \n",
        "  def value_iteration(self, delta, gamma):\n",
        "    #solve MDP\n",
        "    \n",
        "    \n",
        "    #initialize V\n",
        "    \n",
        "    \n",
        "    #while True or iterate over max_iterations\n",
        "    while True:\n",
        "      delta = 0\n",
        "      \n",
        "      #iteration counter\n",
        "      iteration += 1\n",
        "    \n",
        "      #keeping track of previous value function\n",
        "      for state in possible_actions:\n",
        "        \n",
        "        old_V = V[state]\n",
        "        \n",
        "        # Synchronously update the state-action value\n",
        "        action_values = q\n",
        "        \n",
        "        # select best action to perform based on highest state-action value\n",
        "        best_action_value = np.max(action_values)\n",
        "        \n",
        "        #updtae current state_value function\n",
        "        \n",
        "        #V[state] = max(sum([p *(R + gamma * V[state]) for p, s, r in transition states probabilities])\n",
        "        \n",
        "        V[state] = best_action_value               \n",
        "\n",
        "        delta = max(delta, np.abs(V[state] - old_V[state]) #stopping criteria\n",
        "                    \n",
        "        if delta < epsilon * (1-gamma)/ gamma:   #epsilon --> small enough\n",
        "          print(\"FINAL RESULTS:\")\n",
        "                  print(\"Iterations: \" + str(iteration))\n",
        "                  print(\"Delta: \" + str(delta))\n",
        "                  print(\"Gamma: \" + str(gamma))\n",
        "                  print(\"Epsilon: \" + str(epsilon))\n",
        "          break\n",
        "                    \n",
        "        # intialize optimal policy\n",
        "     optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
        "    \n",
        "     # update the optimal polciy according to optimal value function 'V'\n",
        "     optimal_policy = update_policy(env, optimal_policy, V, discount_factor)\n",
        "    \n",
        "     return V, optimal_policy\n",
        "                    \n",
        "                    \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        V = max(transition_prob * (R + gamma * V(s')))\n",
        "  \n",
        "  #Bellman Equation: transition_prob * (R + gamma * V(s'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEs_-Pv-HOaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# while True:\n",
        "#         delta = 0\n",
        "#         u = u1.copy()\n",
        "#         iteration += 1\n",
        "#         graph_list.append(u)\n",
        "#         for s in range(tot_states):\n",
        "#             reward = r[s]\n",
        "#             v = np.zeros((1,tot_states))\n",
        "#             v[0,s] = 1.0\n",
        "#             u1[s] = return_state_utility(v, T, u, reward, gamma)\n",
        "#             delta = max(delta, np.abs(u1[s] - u[s])) #Stopping criteria       \n",
        "#         if delta < epsilon * (1 - gamma) / gamma:\n",
        "\n",
        "\n",
        "\n",
        " def return_state_utility(v, T, u, reward, gamma):\n",
        "    \"\"\"Return the state utility.\n",
        "\n",
        "    @param v the state vector\n",
        "    @param T transition matrix\n",
        "    @param u utility vector\n",
        "    @param reward for that state\n",
        "    @param gamma discount factor\n",
        "    @return the utility of the state\n",
        "    \"\"\"\n",
        "    action_array = np.zeros(4)\n",
        "    \n",
        "    for action in range(0, 4):\n",
        "        action_array[action] = np.sum(np.multiply(u, np.dot(v, T[:,:,action])))\n",
        "    return reward + gamma * np.max(action_array)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}