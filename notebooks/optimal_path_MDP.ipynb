{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimal_path_MDP.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaime-garvey/web_personalization_portfolio/blob/master/notebooks/optimal_path_MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PDlqZkswvK",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34AIJymyJjmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQydi0t7s8NX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dtj1oeMs0uK",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffEQkAypRCBw",
        "colab_type": "text"
      },
      "source": [
        "**Data Structures**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHyof4hlnu3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id_id2page_dict = \"1DGeJYGuF5fTnruZz84ZaBGelakRcKiKf\"\n",
        "id_possible_actions_dict = \"15WUaciYffXqNN9_3t3TGWXJ5FotPWtx0\"\n",
        "#id_transition_matrix = \"1Fcph1OO4GUMCeObMleRWX_FnPLSWAuMs\"\n",
        "id_rewards_dict = \"1CPKK-kriTdUY1VtWfdCAA3HvZklEa116\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvCl1rzVtdN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_pickle_from_drive(id):\n",
        "  downloaded = drive.CreateFile({'id':id}) \n",
        "  downloaded.GetContentFile('Filename.csv')  \n",
        "  data = pickle.load(open('Filename.csv', 'rb'))\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjfzqG2ruirc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2page_dict = load_pickle_from_drive(id_id2page_dict)\n",
        "possible_actions= load_pickle_from_drive(id_possible_actions_dict)\n",
        "#transition_matrix = load_pickle_from_drive(id_transition_matrix)\n",
        "rewards_dict = load_pickle_from_drive(id_rewards_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdKqfN4jZoY5",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt1v92M7Zqjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "terminals = [k for k,v in rewards_dict.items() if v == 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5ljbLKfb5Ce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "possible_actions = possible_actions.to_dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJG_182Itlkr",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXKDPBI8Y1Vc",
        "colab_type": "text"
      },
      "source": [
        "## MDP Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdA0FtyKDEAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MDP:\n",
        "  '''\n",
        "  state_0 --> initial state\n",
        "  terminal_states --> terminal state\n",
        "  possible_actions --> all actions for every state {state1: {state2, tp}...}\n",
        "  '''\n",
        "  \n",
        "  def __init__(self, state, possible_actions=possible_actions, rewards_dict=rewards_dict, gamma=0.9):\n",
        "    self.state = state\n",
        "    self.possible_actions = possible_actions\n",
        "    self.rewards_dict = rewards_dict\n",
        "    self.gamma = gamma\n",
        "    self.terminals = [k for k,v in rewards_dict.items() if v == 1]\n",
        "     \n",
        "#       update(self,\n",
        "#              states_possible_actions=states_possible_actions,\n",
        "#              terminal_states=terminal_states,\n",
        "#              states = set(), \n",
        "#              reward={})\n",
        "      \n",
        "  def reward(self):\n",
        "    #return numeric reward for state\n",
        "    return self.rewards_dict[self.state]\n",
        "  \n",
        "  def actions(self):\n",
        "    #takes in state and returns list of possible actions if not a terminal state\n",
        "    #change if needed\n",
        "    self\n",
        "    if self.rewards_dict[self.state] == 1:\n",
        "      return [None]\n",
        "    else:\n",
        "      #self.possible_actions = list(self.possible_actions[self.state].keys())\n",
        "      self.possible_actions = possible_actions[self.state]\n",
        "      return self.possible_actions\n",
        "\n",
        "#   def transition(state, action):\n",
        "#     #transition probability from state_i to state_j\n",
        "#     for action in self.possible_actions:\n",
        "      \n",
        "#       possible_actions[state][state_j]\n",
        "    \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-BeUql2mIu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(policy.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqMx94seYyKr",
        "colab_type": "text"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loWC8LltgP1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Helper Function to itialize Value Function\n",
        "\n",
        "def initialize_values(rewards_dict=rewards_dict):\n",
        "    #function to inital values at 0\n",
        "    # V(s) only has value if it's not a terminal state\n",
        "    \n",
        "    V={}\n",
        "    \n",
        "    for state in list(rewards_dict.keys()):\n",
        "      if rewards_dict[state] == 1:\n",
        "        V[state] = 0\n",
        "      else:\n",
        "        V[state] = np.random.random()\n",
        "    \n",
        "    return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aiiVTdJntK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_policy(possible_actions=possible_actions, terminals=terminals):\n",
        "  policy = {}\n",
        "  \n",
        "  def key_maxval(state):\n",
        "      \n",
        "      v=list(possible_actions[state].values())\n",
        "      k=list(possible_actions[state].keys())\n",
        "  \n",
        "      return k[v.index(max(v))]\n",
        "    \n",
        "  for state in list(possible_actions.keys()):\n",
        "    #if state not in terminals:\n",
        "    policy[state] = key_maxval(state)\n",
        "    \n",
        "  return policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKPiBZwlp9kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize Policy (state --> action)\n",
        "policy = initialize_policy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5nrVUEBrNKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize Value (future rewards)\n",
        "V = initialize_values()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAXxlCx0raRA",
        "colab_type": "text"
      },
      "source": [
        "# Value Iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwbKhPe28wVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(V=V, gamma=0.9, epsilon=0.001):\n",
        "  while True:\n",
        "    \n",
        "    old_V = V.copy()\n",
        "    delta = 0\n",
        "    # V(s) only has value if it's not a terminal state\n",
        "    for state in list(policy.keys()):\n",
        "      new_v = float('-inf')\n",
        "      try:\n",
        "        #loop and take max value for action \n",
        "        for action in list(MDP(state).actions().keys()):\n",
        "          r = MDP(action).reward()\n",
        "          v = r + gamma * V[action]\n",
        "          if v > new_v:\n",
        "            new_v = v\n",
        "        V[state] = new_v\n",
        "        delta = max(delta, np.abs(old_v = V[state]))\n",
        "        \n",
        "      except:\n",
        "        pass\n",
        "    if delta < epsilon:\n",
        "      break\n",
        "         \n",
        "    #find optimal policy\n",
        "        \n",
        "    for state in list(policy.keys()):\n",
        "      best_action = None\n",
        "      best_value = float('-inf')\n",
        "            \n",
        "            \n",
        "      # loop through all possible actions to find the best current action\n",
        "      try:\n",
        "        for action in list(MDP(state).actions().keys()):\n",
        "          v = r + gamma * V[action]\n",
        "          r = MDP(action).reward()\n",
        "\n",
        "          if v>best_value:\n",
        "            best_value=v\n",
        "            best_a = action\n",
        "        policy[state] = best_a\n",
        "      except:\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS15EWOAzbtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "value_iteration()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KA9o2ed9jgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = open('policy.pkl', 'wb')\n",
        "pickle.dump(policy,output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jkAEHfsHJoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('policy.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFYnx9UjAJvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = open('V.pkl', 'wb')\n",
        "pickle.dump(V,output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0itq91BqHUrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('V.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiWLQZ9Gavxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_lookahead(state=None, V=None, discount_factor = 0.9):\n",
        "    \"\"\"\n",
        "    Helper function to  calculate state-value function\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object\n",
        "        state: state to consider\n",
        "        V: Estimated Value for each state. Vector of length nS\n",
        "        discount_factor: MDP discount factor\n",
        "        \n",
        "    Return:\n",
        "        action_values: Expected value of each action in a state. Vector of length nA\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize vector of action values\n",
        "    V = initialize_values()\n",
        "    \n",
        "    # loop over the actions we can take in an enviorment \n",
        "    for action,probability in MDP(state).actions().items():\n",
        "      reward = MDP(action)\n",
        "      \n",
        "        # loop over the P_sa distribution.\n",
        "        for probablity, next_state, reward, info in env.P[state][action]:\n",
        "             #if we are in state s and take action a. then sum over all the possible states we can land into.\n",
        "            action_values[action] += probablity * (reward + (discount_factor * V[next_state]))\n",
        "            \n",
        "    return action_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plo5uHNYi4On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_policy(env, policy, V, discount_factor):\n",
        "    \n",
        "    \"\"\"\n",
        "    Helper function to update a given policy based on given value function.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        policy: policy to update.\n",
        "        V: Estimated Value for each state. Vector of length nS.\n",
        "        discount_factor: MDP discount factor.\n",
        "    Return:\n",
        "        policy: Updated policy based on the given state-Value function 'V'.\n",
        "    \"\"\"\n",
        "    \n",
        "    for state in range(env.nS):\n",
        "        # for a given state compute state-action value.\n",
        "        action_values = one_step_lookahead(env, state, V, discount_factor)\n",
        "        \n",
        "        # choose the action which maximizez the state-action value.\n",
        "        policy[state] =  np.argmax(action_values)\n",
        "        \n",
        "    return policy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulperLWsi53n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(env, discount_factor = 0.999, max_iteration = 1000):\n",
        "    \"\"\"\n",
        "    Algorithm to solve MPD.\n",
        "    \n",
        "    Arguments:\n",
        "        env: openAI GYM Enviorment object.\n",
        "        discount_factor: MDP discount factor.\n",
        "        max_iteration: Maximum No.  of iterations to run.\n",
        "        \n",
        "    Return:\n",
        "        V: Optimal state-Value function. Vector of lenth nS.\n",
        "        optimal_policy: Optimal policy. Vector of length nS.\n",
        "    \n",
        "    \"\"\"\n",
        "    # intialize value fucntion\n",
        "    V = np.zeros(env.nS)\n",
        "    \n",
        "    # iterate over max_iterations\n",
        "    for i in range(max_iteration):\n",
        "        \n",
        "        #  keep track of change with previous value function\n",
        "        prev_v = np.copy(V) \n",
        "    \n",
        "        # loop over all states\n",
        "        for state in range(env.nS):\n",
        "            \n",
        "            # Asynchronously update the state-action value\n",
        "            #action_values = one_step_lookahead(env, state, V, discount_factor)\n",
        "            \n",
        "            # Synchronously update the state-action value\n",
        "            action_values = one_step_lookahead(env, state, prev_v, discount_factor)\n",
        "            \n",
        "            # select best action to perform based on highest state-action value\n",
        "            best_action_value = np.max(action_values)\n",
        "            \n",
        "            # update the current state-value fucntion\n",
        "            V[state] =  best_action_value\n",
        "            \n",
        "        # if policy not changed over 10 iterations it converged.\n",
        "        if i % 10 == 0:\n",
        "            # if values of 'V' not changing after one iteration\n",
        "            if (np.all(np.isclose(V, prev_v))):\n",
        "                print('Value converged at iteration %d' %(i+1))\n",
        "                break\n",
        "\n",
        "    # intialize optimal policy\n",
        "    optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
        "    \n",
        "    # update the optimal polciy according to optimal value function 'V'\n",
        "    optimal_policy = update_policy(env, optimal_policy, V, discount_factor)\n",
        "    \n",
        "    return V, optimal_policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoPoRazBixtD",
        "colab_type": "text"
      },
      "source": [
        "#### CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvJPjGux3f_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Value(MDP):\n",
        "  '''\n",
        "  states_actions --> dictionary {state_i: {state_j : transition_probability}}\n",
        "  id2pages --> dictionary {page id: \"Page Title\"}\n",
        "  rewards --> dictionary {state: reward value}\n",
        "  theta --> threshold for the amount of iterations\n",
        "  \n",
        "  \n",
        "  '''\n",
        "  V = initialize_values()\n",
        "  \n",
        "  def __init__(self, theta=0.00001):\n",
        "\n",
        "    super().__init__(state_i, possible_actions, terminal_states, gamma)\n",
        "    \n",
        "    self.theta = threshold\n",
        "    self.gamma = gamma\n",
        "    \n",
        "  def initialize_values(self):\n",
        "    #function to inital values at 0\n",
        "    # V(s) only has value if it's not a terminal state\n",
        "    \n",
        "    for page in id2pages.key:\n",
        "      \n",
        "      if page in actions():\n",
        "        \n",
        "        self.V[page] = np.random.random()\n",
        "      else:\n",
        "        self.V[page] = 0      #terminal state\n",
        "        \n",
        "      return self.V\n",
        "  \n",
        "  \n",
        "  def state_action(self, state, gamma):\n",
        "    #Helper function to caluclate Q --> state_value function\n",
        "    \n",
        "    \n",
        "    for action in range(length(self.possible_actions)):\n",
        "      #loop though all possible actions\n",
        "      \n",
        "      for probability, next_state, reward in statespace??\n",
        "      #if we are in state s and take action a. then sum over all \n",
        "      #the possible states we can land into.\n",
        "      \n",
        "        state_action[action] += probability * (reward + (discount_factor * V[next_state]))\n",
        "        \n",
        "    return q\n",
        "        \n",
        "      \n",
        "  def update_policy(self, policy, V, gamma):\n",
        "    #Function to update a given policy based on a given value function\n",
        "    \n",
        "    for state in range(states.keys):\n",
        "      action_values = q\n",
        "      \n",
        "      policy[state] = np.argmax(action_values)\n",
        "    \n",
        "      \n",
        "  def value_iteration(self, delta, gamma):\n",
        "    #solve MDP\n",
        "    \n",
        "    \n",
        "    #initialize V\n",
        "    \n",
        "    \n",
        "    #while True or iterate over max_iterations\n",
        "    while True:\n",
        "      delta = 0\n",
        "      \n",
        "      #iteration counter\n",
        "      iteration += 1\n",
        "    \n",
        "      #keeping track of previous value function\n",
        "      for state in possible_actions:\n",
        "        \n",
        "        old_V = V[state]\n",
        "        \n",
        "        # Synchronously update the state-action value\n",
        "        action_values = q\n",
        "        \n",
        "        # select best action to perform based on highest state-action value\n",
        "        best_action_value = np.max(action_values)\n",
        "        \n",
        "        #updtae current state_value function\n",
        "        \n",
        "        #V[state] = max(sum([p *(R + gamma * V[state]) for p, s, r in transition states probabilities])\n",
        "        \n",
        "        V[state] = best_action_value               \n",
        "\n",
        "        delta = max(delta, np.abs(V[state] - old_V[state]) #stopping criteria\n",
        "                    \n",
        "        if delta < epsilon * (1-gamma)/ gamma:   #epsilon --> small enough\n",
        "          print(\"FINAL RESULTS:\")\n",
        "                  print(\"Iterations: \" + str(iteration))\n",
        "                  print(\"Delta: \" + str(delta))\n",
        "                  print(\"Gamma: \" + str(gamma))\n",
        "                  print(\"Epsilon: \" + str(epsilon))\n",
        "          break\n",
        "                    \n",
        "        # intialize optimal policy\n",
        "     optimal_policy = np.zeros(env.nS, dtype = 'int8')\n",
        "    \n",
        "     # update the optimal polciy according to optimal value function 'V'\n",
        "     optimal_policy = update_policy(env, optimal_policy, V, discount_factor)\n",
        "    \n",
        "     return V, optimal_policy\n",
        "                    \n",
        "                    \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        V = max(transition_prob * (R + gamma * V(s')))\n",
        "  \n",
        "  #Bellman Equation: transition_prob * (R + gamma * V(s'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEs_-Pv-HOaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# while True:\n",
        "#         delta = 0\n",
        "#         u = u1.copy()\n",
        "#         iteration += 1\n",
        "#         graph_list.append(u)\n",
        "#         for s in range(tot_states):\n",
        "#             reward = r[s]\n",
        "#             v = np.zeros((1,tot_states))\n",
        "#             v[0,s] = 1.0\n",
        "#             u1[s] = return_state_utility(v, T, u, reward, gamma)\n",
        "#             delta = max(delta, np.abs(u1[s] - u[s])) #Stopping criteria       \n",
        "#         if delta < epsilon * (1 - gamma) / gamma:\n",
        "\n",
        "\n",
        "\n",
        " def return_state_utility(v, T, u, reward, gamma):\n",
        "    \"\"\"Return the state utility.\n",
        "\n",
        "    @param v the state vector\n",
        "    @param T transition matrix\n",
        "    @param u utility vector\n",
        "    @param reward for that state\n",
        "    @param gamma discount factor\n",
        "    @return the utility of the state\n",
        "    \"\"\"\n",
        "    action_array = np.zeros(4)\n",
        "    \n",
        "    for action in range(0, 4):\n",
        "        action_array[action] = np.sum(np.multiply(u, np.dot(v, T[:,:,action])))\n",
        "    return reward + gamma * np.max(action_array)\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}